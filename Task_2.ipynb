{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1lIe1xeVsQo-Ysxrtg3x0xeCoA1ZoFIg1",
      "authorship_tag": "ABX9TyPPxP7wUp7t6PQoODko2wih",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ahdali24/SP-XD/blob/main/Task_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "zmnKaT_iDCjp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88997bcf-2419-4436-80bf-bb64628da66e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "عدد الصفوف: 286467\n",
            "Label\n",
            "PortScan    158930\n",
            "BENIGN      127537\n",
            "Name: count, dtype: int64\n",
            "\n",
            "عمود الlabel المستخدم: Label\n",
            "\n",
            "قيمة_counts للـ labels (أول 50):\n",
            "Label\n",
            "PortScan    158930\n",
            "BENIGN      127537\n",
            "Name: count, dtype: int64\n",
            "\n",
            "التسميات التي تم اعتبارها PortScan (مكتشفة تلقائياً): ['PortScan']\n",
            "\n",
            "توزيع الفئات (PortScan vs Others): {np.int64(0): np.int64(127537), np.int64(1): np.int64(158930)}\n",
            "\n",
            "الميزات المتاحة من القالب: ['Destination Port', 'Flow Duration', 'Init_Win_bytes_forward', 'Init_Win_bytes_backward', 'Subflow Fwd Bytes', 'Subflow Bwd Bytes', 'act_data_pkt_fwd', 'min_seg_size_forward', 'Total Fwd Packets', 'Flow Bytes/s', 'Flow Packets/s', 'Total Length of Fwd Packets', 'Total Length of Bwd Packets']\n",
            "لم يتم العثور على Source IP أو Destination Port بأسماء متوقعة -> سيتم تجاهل unique_dst_ports_per_src\n",
            "\n",
            "نسبة القيم الفارغة لكل feature قبل التعويض:\n",
            "Destination Port               0.000000\n",
            "Flow Duration                  0.000000\n",
            "Init_Win_bytes_forward         0.000000\n",
            "Init_Win_bytes_backward        0.000000\n",
            "Subflow Fwd Bytes              0.000000\n",
            "Subflow Bwd Bytes              0.000000\n",
            "act_data_pkt_fwd               0.000000\n",
            "min_seg_size_forward           0.000000\n",
            "Total Fwd Packets              0.000000\n",
            "Flow Bytes/s                   0.000052\n",
            "Flow Packets/s                 0.000000\n",
            "Total Length of Fwd Packets    0.000000\n",
            "Total Length of Bwd Packets    0.000000\n",
            "dtype: float64\n",
            "\n",
            "بعد التعويض، أي قيم فارغة؟ 0\n",
            "\n",
            "توزيع الفئات قبل resampling: Counter({1: 158930, 0: 127537})\n",
            "الفئات متقاربة نسبياً أو الفئة الهجومية ليست قليلة جداً - سنستخدم undersample بسيط للفئة الكبرى\n",
            "شكل البيانات بعد الresample: (255074, 13)\n",
            "توزيع الفئات بعد الresample: {np.int64(0): np.int64(127537), np.int64(1): np.int64(127537)}\n",
            "\n",
            "Accuracy: 0.9999059103953332\n",
            "Confusion Matrix:\n",
            " [[31883     2]\n",
            " [    4 31880]]\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9999    0.9999    0.9999     31885\n",
            "           1     0.9999    0.9999    0.9999     31884\n",
            "\n",
            "    accuracy                         0.9999     63769\n",
            "   macro avg     0.9999    0.9999    0.9999     63769\n",
            "weighted avg     0.9999    0.9999    0.9999     63769\n",
            "\n",
            "\n",
            "أهم الميزات:\n",
            "                    feature  importance\n",
            "          Subflow Fwd Bytes    0.201093\n",
            "Total Length of Fwd Packets    0.169369\n",
            "              Flow Duration    0.138011\n",
            "          Total Fwd Packets    0.094477\n",
            "     Init_Win_bytes_forward    0.078634\n",
            "             Flow Packets/s    0.072148\n",
            "Total Length of Bwd Packets    0.070958\n",
            "           Destination Port    0.054602\n",
            "          Subflow Bwd Bytes    0.036370\n",
            "               Flow Bytes/s    0.034194\n",
            "    Init_Win_bytes_backward    0.023806\n",
            "       min_seg_size_forward    0.013430\n",
            "           act_data_pkt_fwd    0.012908\n",
            "\n",
            "تم حفظ الموديل والـ scaler في /content/ (rf_portscan_model.joblib, scaler_portscan.joblib)\n"
          ]
        }
      ],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "# جاهز للتشغيل في Colab / Kaggle / Local\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "import joblib\n",
        "\n",
        "# === مسار الملف - عدّليه لو لازم ===\n",
        "CSV_PATH = \"/content/Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv\"\n",
        "\n",
        "# === قراءة الملف ===\n",
        "df = pd.read_csv(CSV_PATH, sep=',')\n",
        "df.columns = df.columns.str.strip()   # تنظيف المسافات حول أسماء الأعمدة\n",
        "print(\"عدد الصفوف:\", len(df))\n",
        "\n",
        "# === 1) نشوف كل الفئات المتاحة في العمود Label (أو أي اسم مطابق) ===\n",
        "df.columns = df.columns.str.strip()\n",
        "label_col = \"Label\"\n",
        "print(df[label_col].value_counts())\n",
        "\n",
        "\n",
        "print(\"\\nعمود الlabel المستخدم:\", label_col)\n",
        "print(\"\\nقيمة_counts للـ labels (أول 50):\")\n",
        "print(df[label_col].astype(str).str.strip().value_counts().head(50))\n",
        "\n",
        "# === 2) اكتشاف تسميات PortScan تلقائياً ===\n",
        "# نبحث عن أي label تحتوي على كلمة 'port' أو 'scan' (case-insensitive)\n",
        "labels = df[label_col].astype(str).str.strip()\n",
        "portscan_mask = labels.str.lower().str.contains(\"port\") | labels.str.lower().str.contains(\"scan\")\n",
        "portscan_labels = sorted(labels[portscan_mask].unique().tolist())\n",
        "print(\"\\nالتسميات التي تم اعتبارها PortScan (مكتشفة تلقائياً):\", portscan_labels)\n",
        "\n",
        "if len(portscan_labels) == 0:\n",
        "    # لو مفيش، خلي المستخدم يحدد اسم الهجوم يدوياً\n",
        "    raise ValueError(\"لم أجد أي تسميات تبدو كـ PortScan. شغّل print(df[label_col].value_counts()) وتأكد من الأسماء.\")\n",
        "\n",
        "# === 3) نعمل target binary: 1 = PortScan، 0 = باقي ===\n",
        "y = labels.apply(lambda x: 1 if x in portscan_labels else 0)\n",
        "print(\"\\nتوزيع الفئات (PortScan vs Others):\", dict(zip(*np.unique(y, return_counts=True))))\n",
        "\n",
        "# === 4) اختيار الميزات (تأكدي تعديل الأسماء لو عندك اختلاف) ===\n",
        "base_features = [\n",
        "    \"Destination Port\", \"Flow Duration\",\n",
        "    \"Init_Win_bytes_forward\", \"Init_Win_bytes_backward\",\n",
        "    \"Subflow Fwd Bytes\", \"Subflow Bwd Bytes\",\n",
        "    \"act_data_pkt_fwd\", \"min_seg_size_forward\",\n",
        "    # شوية أعمدة شائعة في CIC-IDS2017 (لو موجودة)\n",
        "    \"Total Fwd Packets\", \"Total Bwd Packets\", \"Flow Bytes/s\", \"Flow Packets/s\",\n",
        "    \"Total Length of Fwd Packets\", \"Total Length of Bwd Packets\"\n",
        "]\n",
        "feature_cols = [c for c in base_features if c in df.columns]\n",
        "print(\"\\nالميزات المتاحة من القالب:\", feature_cols)\n",
        "\n",
        "# === 5) Feature engineering إضافية ===\n",
        "X = df[feature_cols].copy()\n",
        "\n",
        "# 5.1 مجموع الباكتس لكل فلو لو موجودين\n",
        "if \"Total Fwd Packets\" in df.columns and \"Total Bwd Packets\" in df.columns:\n",
        "    X[\"packets_per_flow\"] = pd.to_numeric(df[\"Total Fwd Packets\"], errors='coerce').fillna(0) + \\\n",
        "                            pd.to_numeric(df[\"Total Bwd Packets\"], errors='coerce').fillna(0)\n",
        "\n",
        "# 5.2 bytes per packet إن أمكن\n",
        "# نحاول استخدام Flow Bytes/s أو مجموع الأطوال\n",
        "if \"Flow Bytes/s\" in df.columns and \"packets_per_flow\" in X.columns:\n",
        "    # لاحظ: Flow Bytes/s قد يكون معدل، لكن نجرب حساب bytes_per_packet بتقدير بسيط\n",
        "    X[\"bytes_per_packet\"] = pd.to_numeric(df.get(\"Flow Bytes/s\", 0), errors='coerce') / (X[\"packets_per_flow\"].replace(0, np.nan))\n",
        "else:\n",
        "    # بالتناوب: استخدام مجموع أطوال الـ fwd/bwd مقسوم على packets\n",
        "    if \"Total Length of Fwd Packets\" in df.columns and \"Total Length of Bwd Packets\" in df.columns and \"packets_per_flow\" in X.columns:\n",
        "        X[\"bytes_per_packet\"] = (pd.to_numeric(df[\"Total Length of Fwd Packets\"], errors='coerce').fillna(0) +\n",
        "                                 pd.to_numeric(df[\"Total Length of Bwd Packets\"], errors='coerce').fillna(0)) / X[\"packets_per_flow\"].replace(0, np.nan)\n",
        "\n",
        "# 5.3 unique_dst_ports_per_src (لو فيه Source IP و Destination Port)\n",
        "src_ip_cols = [c for c in df.columns if c.lower().replace(\" \", \"\") in (\"sourceip\",\"srcip\",\"source ip\",\"src ip\",\"source_ip\",\"src_ip\")]\n",
        "dst_port_col = None\n",
        "for name in [\"Destination Port\", \"Dst Port\", \"DstPort\", \"DestinationPort\", \"Dst Port Number\"]:\n",
        "    if name in df.columns:\n",
        "        dst_port_col = name\n",
        "        break\n",
        "if len(src_ip_cols) > 0 and dst_port_col is not None:\n",
        "    src_col = src_ip_cols[0]\n",
        "    print(\"\\nاستخدام عمود الـ Source IP:\", src_col)\n",
        "    # نعمل aggregation: لكل source ip نعد عدد المنافذ المقصودة المختلفة خلال الdataset\n",
        "    tmp = df[[src_col, dst_port_col]].copy()\n",
        "    # نعالج القيم المفقودة\n",
        "    tmp = tmp.dropna(subset=[src_col, dst_port_col])\n",
        "    ports_per_src = tmp.groupby(src_col)[dst_port_col].nunique().rename(\"unique_dst_ports_per_src\")\n",
        "    # نربطه بالـ X عبر merge على الفهرس الأصلي\n",
        "    ports_per_src_df = tmp.merge(ports_per_src.reset_index(), on=src_col, how='left')\n",
        "    # نحاول الانضمام: أفضل طريقة هي map من الـ src_col للقيمة\n",
        "    map_ports = ports_per_src.to_dict()\n",
        "    X[\"unique_dst_ports_per_src\"] = df[src_col].map(map_ports).fillna(0)\n",
        "else:\n",
        "    print(\"لم يتم العثور على Source IP أو Destination Port بأسماء متوقعة -> سيتم تجاهل unique_dst_ports_per_src\")\n",
        "\n",
        "# === 6) تحويل لأرقام والتعامل مع القيم الفارغة ===\n",
        "X = X.apply(pd.to_numeric, errors='coerce')\n",
        "print(\"\\nنسبة القيم الفارغة لكل feature قبل التعويض:\")\n",
        "print(X.isna().mean())\n",
        "\n",
        "# نعوّض بالقيمة المتوسطة لكل عمود\n",
        "X = X.fillna(X.mean())\n",
        "\n",
        "print(\"\\nبعد التعويض، أي قيم فارغة؟\", X.isna().sum().sum())\n",
        "\n",
        "# === 7) Scaling ===\n",
        "# معالجة القيم اللانهائية والقيم الكبيرة جدًا\n",
        "X = X.replace([np.inf, -np.inf], np.nan)\n",
        "X = X.fillna(X.mean())\n",
        "\n",
        "# Scaling\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# === 8) التعامل مع عدم التوازن ===\n",
        "# لو PortScan قليل جدا نستخدم SMOTE، وإلا نستخدم undersample للفئة الكبيرة\n",
        "from collections import Counter\n",
        "print(\"\\nتوزيع الفئات قبل resampling:\", Counter(y))\n",
        "minor_count = Counter(y)[1]\n",
        "major_count = Counter(y)[0]\n",
        "\n",
        "if minor_count == 0:\n",
        "    raise ValueError(\"لا توجد أمثلة لفئة PortScan بعد الفحص التلقائي.\")\n",
        "if minor_count < 0.1 * major_count:\n",
        "    print(\"الفئة الهجومية قليلة - سنستخدم SMOTE لعمل oversample\")\n",
        "    sm = SMOTE(random_state=42)\n",
        "    X_res, y_res = sm.fit_resample(X_scaled, y)\n",
        "else:\n",
        "    print(\"الفئات متقاربة نسبياً أو الفئة الهجومية ليست قليلة جداً - سنستخدم undersample بسيط للفئة الكبرى\")\n",
        "    rus = RandomUnderSampler(random_state=42)\n",
        "    X_res, y_res = rus.fit_resample(X_scaled, y)\n",
        "\n",
        "print(\"شكل البيانات بعد الresample:\", X_res.shape)\n",
        "print(\"توزيع الفئات بعد الresample:\", dict(zip(*np.unique(y_res, return_counts=True))))\n",
        "\n",
        "# === 9) split + train ===\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_res, y_res, test_size=0.25, random_state=42, stratify=y_res)\n",
        "\n",
        "model = RandomForestClassifier(n_estimators=200, random_state=42, class_weight='balanced')\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# === 10) تقييم ===\n",
        "print(\"\\nAccuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred, digits=4))\n",
        "\n",
        "# نعرض أهم الميزات\n",
        "try:\n",
        "    importances = model.feature_importances_\n",
        "    feat_names = X.columns.tolist()\n",
        "    feat_imp_df = pd.DataFrame({\"feature\": feat_names, \"importance\": importances}).sort_values(\"importance\", ascending=False)\n",
        "    print(\"\\nأهم الميزات:\")\n",
        "    print(feat_imp_df.head(20).to_string(index=False))\n",
        "except Exception as e:\n",
        "    print(\"خطأ عند حساب أهمية الميزات:\", e)\n",
        "\n",
        "# === 11) حفظ الموديل والـ scaler ===\n",
        "joblib.dump(model, \"/content/rf_portscan_model.joblib\")\n",
        "joblib.dump(scaler, \"/content/scaler_portscan.joblib\")\n",
        "print(\"\\nتم حفظ الموديل والـ scaler في /content/ (rf_portscan_model.joblib, scaler_portscan.joblib)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "v6dXJnX2JnZ4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7568902c-bf27-4dc6-b03d-b6e3d4051ce5"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    }
  ]
}